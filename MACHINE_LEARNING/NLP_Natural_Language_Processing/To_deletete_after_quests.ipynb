{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'Cette notion de fréquence constitue la base de la base du text mining.'\n",
    "nltk.FreqDist(string.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**preprocessing** = nettoyage du texte et action de traitement\n",
    "\n",
    "**tokenizing** = découpage en token\n",
    "\n",
    "**tokens** = mots, ponctuation, phrases entières... tout dépend de notre objectif de NLP\n",
    "\n",
    "**stemming** = racinisation, cad prends la racine des mots en les tronquants.\n",
    "\n",
    "**lemmatizing** = comme le stemming mais avec des règles grammaticales. En fonction des situations l'un peut $etre plus pertinant que l'autre.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le **lemmatizing** permet de conserver un lemma, qu'on pourrait traduire par racine, qui est plus proche du vrai sens du mot. Par exemple, le lemma de did devrait être do, celui de am devrait être be. Il ne suffit donc pas de couper les mots, mais de retrouver son origine. Un lemmatizer s'appuie donc sur le travail de linguistes pour créer les règles. Heureusement pour nous, il existe déjà des lemmatizers bien paramétrés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aline\n",
    "\n",
    "\n",
    "#on change la case\n",
    "article_lower = article.lower()\n",
    "#on enlève la ponctuation\n",
    "article_ponct = re.sub(r'[^\\w\\s]','',article_lower)\n",
    "\n",
    "#on re-sépare l'article en mots\n",
    "article_word_1 = nltk.word_tokenize(article_ponct)\n",
    "\n",
    "#on enève les stopwords\n",
    "article_clean = []\n",
    "\n",
    "for words in article_word_1:\n",
    "\n",
    "  if words not in nltk.corpus.stopwords.words(\"english\"):\n",
    "\n",
    "    article_clean.append(words)\n",
    "\n",
    "#on compte le nombre d'occurence\n",
    "dico_frequency = nltk.FreqDist(article_clean)\n",
    "\n",
    "#transforme le dictionnaire en DataFrame\n",
    "df_frequency = pd.DataFrame.from_dict(dico_frequency, orient = 'index').reset_index()\n",
    "df_frequency.rename({'index' :'word', 0 : 'frequency' }, axis = 1, inplace=True)\n",
    "\n",
    "#Récupère les 20 mots les plus fréquents et les affiche sous forme de graphique\n",
    "df_frequency.sort_values(by = 'frequency',ascending = False).head(20).plot.bar(x= 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Christophe\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#Supprime la ponctuation\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "token_sansp = tokenizer.tokenize(article.lower())\n",
    "\n",
    "#COmpte la fréquence\n",
    "frequence2 = nltk.FreqDist(token_sansp)\n",
    "\n",
    "#Crée le Dataframe\n",
    "df4 = pd.DataFrame(list(frequence2.items()), columns=['Mots', 'Frequence'])\n",
    "\n",
    "#Enlève les stopwords\n",
    "sw = nltk.corpus.stopwords.words(\"english\")\n",
    "df4.drop(df4.loc[df4['Mots'].isin(sw)].index, inplace=True)\n",
    "\n",
    "#Tri par fréquence\n",
    "df5 = df4.sort_values(by = 'Frequence', ascending = False)\n",
    "df5.reset_index(drop = True, inplace = True)\n",
    "\n",
    "#Affiche le graphique\n",
    "fig = px.bar(df5.iloc[0:20], x='Mots', y='Frequence', width=1000, height = 600, template= 'plotly_dark')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4544d665ae83a938e78316550df4048c4b5eacadf9e9ff52d5a508ec23c5226a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
